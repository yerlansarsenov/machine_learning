{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torchvision\n",
    "from torchvision import transforms as tfs\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [[3, 58.5, 29.9, 73, 3, 4, 6, 1, 1], [1, 33.0, 16.2, 79, 4, 5, 5, 1, 3], [2, 50.0, 21.5, 87, 4, 5, 3, 1, 3], [3, 65.5, 47.0, 52, 2, 3, 2, 1, 1], [3, 86.7, 31.0, 110, 3, 5, 6, 1, 2], [3, 113.6, 51.12, 118, 2, 15, 4, 4, 2], [1, 40.0, 13.5, 118, 5, 5, 6, 1, 2], [1, 33.0, 14.7, 85, 5, 5, 6, 1, 3], [3, 75.5, 34.5, 91, 7, 9, 3, 1, 3], [3, 181.2, 90.6, 110, 3, 6, 1, 4, 4], [3, 115.0, 44.5, 115, 15, 17, 6, 3, 2], [1, 34.0, 12.5, 106, 4, 5, 6, 1, 1], [3, 57.0, 23.0, 76, 4, 4, 5, 1, 3], [3, 80.0, 35.0, 114, 1, 9, 5, 4, 4], [2, 43.0, 18.5, 68, 2, 4, 5, 1, 3], [1, 31.0, 17.5, 83, 2, 5, 2, 1, 3], [1, 39.0, 15.9, 108, 4, 6, 6, 1, 2], [3, 75.0, 45.0, 73, 9, 9, 6, 2, 2], [4, 85.0, 43.9, 88, 2, 5, 5, 4, 3], [2, 45.0, 25.0, 90, 4, 5, 5, 4, 1], [2, 44.0, 21.5, 86, 4, 5, 5, 1, 3], [3, 57.1, 29.0, 81, 5, 5, 6, 1, 3], [1, 21.0, 9.8, 72, 2, 4, 6, 1, 1], [1, 21.0, 9.8, 70, 2, 4, 6, 1, 1], [3, 70.0, 32.5, 88, 6, 9, 3, 1, 3], [3, 61.0, 35.0, 83, 8, 9, 6, 2, 3], [3, 74.4, 32.9, 79, 3, 9, 5, 3, 3], [3, 96.0, 82.0, 118, 12, 16, 6, 1, 2], [3, 72.0, 38.0, 78, 1, 5, 6, 1, 3], [3, 61.4, 26.5, 70, 4, 4, 6, 1, 3], [2, 57.0, 26.0, 99, 1, 9, 6, 1, 3], [3, 60.0, 22.9, 65, 3, 4, 6, 1, 3], [3, 112.8, 46.0, 116, 3, 16, 2, 1, 2], [1, 34.0, 14.5, 85, 3, 5, 3, 1, 3], [1, 24.0, 8.2, 77, 4, 5, 6, 1, 1], [3, 72.0, 32.5, 87, 3, 9, 5, 2, 3], [3, 76.4, 28.5, 115, 7, 9, 6, 2, 2], [3, 81.9, 34.99, 98, 3, 5, 5, 4, 2], [3, 65.3, 33.5, 89, 4, 5, 5, 1, 1], [2, 42.0, 21.0, 78, 3, 5, 5, 1, 3], [1, 31.0, 12.5, 58, 2, 2, 6, 1, 1], [3, 63.0, 26.0, 68, 3, 4, 5, 1, 2], [3, 64.0, 22.0, 85, 1, 3, 5, 1, 1], [2, 43.0, 25.1, 83, 3, 5, 6, 2, 3], [3, 69.0, 46.0, 92, 3, 11, 6, 3, 2], [3, 59.2, 23.9, 69, 3, 4, 5, 1, 3], [1, 30.9, 14.5, 75, 4, 4, 5, 1, 3], [3, 62.6, 22.7, 71, 4, 4, 5, 1, 3], [1, 45.0, 14.9, 110, 9, 9, 5, 1, 2], [1, 41.0, 15.2, 117, 9, 9, 5, 1, 2], [1, 39.0, 13.3, 115, 1, 5, 5, 1, 2], [3, 72.0, 28.5, 119, 7, 10, 6, 1, 2], [5, 130.0, 73.0, 91, 3, 5, 5, 3, 1], [4, 92.0, 60.0, 73, 5, 5, 5, 3, 1], [1, 32.0, 15.0, 63, 2, 4, 3, 1, 3], [3, 75.0, 37.0, 79, 5, 5, 6, 2, 1], [2, 51.0, 23.5, 72, 3, 4, 5, 1, 1], [2, 60.0, 41.0, 80, 2, 5, 3, 4, 1], [4, 104.0, 53.04, 120, 2, 13, 4, 2, 2], [2, 52.0, 22.5, 86, 5, 5, 5, 2, 3], [2, 45.0, 18.5, 80, 3, 5, 3, 1, 3], [2, 68.0, 23.5, 111, 2, 9, 5, 1, 2], [4, 96.0, 25.0, 100, 7, 7, 2, 1, 2], [2, 52.0, 23.5, 82, 4, 5, 3, 1, 3], [1, 45.0, 22.0, 80, 5, 5, 5, 1, 1], [3, 60.0, 23.0, 77, 5, 5, 5, 1, 3], [2, 54.3, 24.5, 115, 2, 8, 5, 1, 2], [3, 75.0, 32.0, 78, 5, 5, 6, 3, 1], [1, 45.1, 19.5, 120, 12, 12, 6, 2, 2], [3, 64.0, 26.5, 80, 5, 5, 3, 1, 2], [2, 70.0, 26.0, 105, 6, 9, 5, 4, 2], [2, 70.0, 34.3, 117, 6, 10, 5, 1, 2], [2, 52.0, 26.0, 75, 5, 8, 5, 4, 3], [4, 86.0, 41.0, 95, 2, 14, 6, 3, 2], [4, 86.0, 41.0, 95, 2, 12, 6, 2, 2], [4, 83.0, 53.0, 76, 5, 6, 3, 4, 1], [3, 66.0, 25.0, 114, 2, 6, 5, 1, 2], [2, 54.0, 23.0, 80, 5, 5, 2, 1, 1], [1, 33.0, 13.5, 89, 5, 5, 3, 1, 1], [3, 99.0, 48.0, 110, 4, 9, 6, 4, 2], [3, 63.0, 25.9, 80, 1, 5, 5, 4, 3], [3, 79.0, 35.0, 75, 1, 5, 5, 1, 1], [3, 60.0, 29.0, 76, 3, 5, 5, 1, 3], [2, 46.0, 24.3, 68, 4, 5, 6, 1, 1], [2, 53.5, 32.0, 110, 14, 16, 6, 2, 2], [2, 48.0, 24.9, 86, 5, 5, 6, 1, 3], [3, 70.0, 32.5, 88, 6, 8, 5, 1, 3], [3, 66.0, 32.5, 88, 6, 8, 3, 1, 3], [2, 64.0, 18.0, 110, 6, 6, 5, 4, 2], [2, 43.6, 18.0, 67, 2, 4, 3, 1, 3], [2, 42.0, 23.8, 62, 3, 4, 5, 1, 1], [3, 60.0, 26.5, 70, 3, 5, 5, 1, 3], [1, 30.0, 17.0, 73, 3, 4, 5, 2, 3], [3, 57.4, 25.5, 62, 4, 4, 6, 1, 3], [2, 43.0, 20.5, 76, 3, 4, 5, 1, 3], [1, 32.4, 18.5, 85, 2, 5, 5, 1, 3], [1, 19.4, 23.0, 94, 2, 5, 5, 2, 3], [1, 48.0, 18.0, 104, 4, 9, 6, 1, 2], [1, 41.0, 18.0, 89, 4, 5, 3, 2, 3], [2, 64.0, 23.0, 109, 4, 6, 6, 1, 2], [4, 74.0, 34.0, 77, 2, 5, 3, 1, 3], [2, 43.0, 19.5, 72, 1, 4, 3, 1, 3], [2, 66.1, 31.5, 108, 8, 12, 5, 1, 2], [2, 51.0, 22.5, 87, 4, 5, 5, 4, 3], [1, 32.0, 17.6, 86, 2, 5, 3, 1, 3], [3, 57.0, 22.5, 80, 5, 5, 3, 1, 3], [4, 85.0, 36.0, 86, 9, 9, 5, 2, 3], [2, 45.0, 20.5, 80, 5, 5, 6, 1, 2], [1, 37.0, 17.5, 62, 2, 4, 6, 1, 1], [2, 53.0, 35.0, 87, 11, 12, 6, 2, 2], [2, 60.0, 32.3, 84, 3, 5, 6, 1, 3], [2, 60.0, 26.9, 94, 9, 9, 3, 2, 3], [2, 52.0, 27.0, 88, 3, 5, 5, 1, 3], [2, 52.0, 22.7, 87, 2, 5, 5, 4, 3], [1, 40.0, 18.0, 87, 8, 9, 5, 1, 3], [3, 66.7, 33.5, 94, 2, 9, 3, 1, 1], [1, 40.0, 20.0, 86, 8, 9, 5, 3, 3], [2, 52.0, 26.9, 88, 3, 5, 5, 1, 3], [3, 58.3, 28.5, 78, 3, 5, 5, 1, 3], [2, 59.4, 28.5, 120, 7, 17, 4, 2, 2], [3, 117.0, 60.0, 85, 2, 5, 6, 1, 1], [2, 46.0, 20.7, 75, 5, 5, 5, 1, 3], [1, 32.0, 16.4, 76, 5, 5, 5, 1, 3], [2, 50.0, 29.0, 80, 3, 5, 6, 3, 3], [3, 70.0, 24.5, 82, 5, 5, 5, 1, 3], [3, 72.0, 30.0, 86, 4, 9, 3, 2, 3], [3, 100.0, 52.0, 69, 4, 5, 5, 3, 1], [3, 123.0, 56.0, 80, 3, 5, 5, 3, 1], [2, 55.0, 23.0, 62, 2, 4, 2, 1, 1], [2, 68.7, 29.999999, 110, 9, 9, 6, 2, 2], [2, 51.0, 24.4, 88, 3, 5, 5, 1, 3], [3, 84.0, 67.0, 78, 2, 5, 5, 3, 1], [3, 90.0, 64.0, 86, 5, 5, 5, 1, 1], [3, 170.0, 135.0, 114, 21, 21, 6, 4, 2], [1, 43.0, 30.0, 85, 2, 5, 5, 2, 1], [1, 36.0, 28.5, 72, 5, 8, 5, 2, 2], [2, 46.0, 17.9, 65, 2, 4, 5, 1, 3], [3, 70.0, 33.4, 89, 4, 5, 5, 3, 3], [1, 43.0, 24.0, 78, 3, 5, 3, 1, 1], [1, 37.0, 19.5, 95, 8, 9, 5, 2, 2], [2, 58.0, 33.9, 76, 3, 5, 6, 4, 1], [4, 135.0, 105.0, 110, 5, 22, 6, 4, 2], [2, 46.0, 25.0, 64, 4, 4, 6, 1, 3], [3, 67.9, 29.0, 117, 5, 12, 6, 2, 2], [3, 72.0, 29.5, 86, 4, 9, 5, 1, 3], [1, 32.0, 15.5, 72, 4, 4, 5, 1, 3], [1, 50.0, 22.3, 106, 4, 9, 5, 2, 2], [2, 43.8, 24.5, 72, 3, 4, 6, 1, 3], [3, 60.0, 24.5, 80, 5, 5, 2, 1, 3], [2, 58.0, 24.0, 85, 5, 5, 5, 2, 3], [3, 93.0, 39.5, 104, 1, 9, 5, 3, 2], [2, 48.0, 20.5, 62, 2, 4, 6, 1, 3], [3, 68.0, 34.8, 93, 3, 5, 6, 2, 3], [3, 61.4, 33.0, 75, 2, 5, 5, 1, 3], [3, 60.0, 22.0, 65, 4, 4, 5, 1, 3], [2, 55.0, 27.3, 85, 4, 5, 3, 4, 1], [1, 35.0, 17.9, 63, 2, 4, 5, 1, 1], [3, 59.0, 31.0, 70, 2, 4, 5, 1, 3], [1, 45.0, 13.5, 65, 2, 4, 5, 1, 1], [2, 45.0, 22.5, 62, 3, 4, 5, 1, 3], [3, 107.0, 79.0, 115, 7, 8, 6, 1, 2], [1, 40.0, 13.5, 107, 9, 9, 5, 1, 2], [4, 92.0, 48.0, 86, 9, 13, 6, 2, 2], [2, 43.0, 23.0, 78, 4, 5, 5, 1, 3], [3, 80.0, 29.0, 108, 10, 10, 6, 1, 1], [1, 42.0, 14.999999, 110, 9, 9, 2, 1, 2], [2, 79.8, 28.0, 114, 12, 18, 6, 2, 2], [3, 60.1, 28.5, 70, 3, 5, 6, 1, 3], [4, 130.0, 42.0, 81, 3, 5, 3, 3, 1], [2, 45.0, 17.8, 64, 2, 4, 5, 1, 3], [3, 78.0, 34.5, 93, 2, 9, 5, 3, 3], [1, 44.0, 14.8, 116, 2, 9, 5, 1, 2], [4, 105.0, 43.5, 86, 5, 9, 6, 2, 3], [2, 48.0, 21.4, 92, 5, 5, 5, 4, 3], [2, 42.0, 22.5, 70, 4, 4, 6, 1, 3], [2, 50.0, 23.5, 80, 2, 5, 5, 2, 1], [4, 100.0, 25.0, 100, 7, 7, 2, 3, 2], [3, 70.0, 32.566, 82, 5, 5, 5, 4, 1], [1, 34.0, 13.8, 75, 5, 5, 5, 1, 3], [2, 59.9, 21.5, 89, 3, 5, 5, 1, 3], [4, 104.0, 59.0, 69, 3, 5, 3, 1, 1], [3, 69.0, 55.0, 40, 3, 3, 6, 1, 1], [4, 250.0, 125.0, 100, 6, 6, 3, 1, 1], [1, 33.0, 14.0, 75, 5, 5, 5, 1, 3], [3, 165.0, 128.0, 97, 6, 9, 5, 1, 1], [2, 44.0, 26.0, 75, 3, 4, 5, 1, 1], [3, 80.0, 32.5, 90, 1, 9, 6, 3, 3], [3, 74.0, 33.5, 77, 3, 9, 5, 3, 3], [1, 32.0, 15.8, 76, 4, 5, 3, 1, 3], [3, 57.0, 29.999, 80, 2, 4, 6, 1, 3], [1, 40.0, 15.8, 117, 6, 9, 5, 1, 2], [2, 50.0, 28.0, 84, 4, 5, 5, 3, 1], [1, 40.0, 13.8, 117, 6, 9, 3, 1, 2], [1, 33.4, 14.0, 74, 5, 5, 5, 1, 3], [2, 40.0, 19.8, 62, 2, 4, 6, 1, 1], [2, 59.8, 20.5, 116, 7, 10, 6, 1, 2], [2, 46.0, 17.9, 65, 2, 4, 5, 1, 3], [1, 34.0, 14.5, 89, 3, 5, 3, 4, 3], [4, 90.0, 30.0, 90, 5, 5, 6, 4, 3], [3, 114.1, 63.5, 116, 4, 16, 6, 2, 2], [1, 37.0, 20.0, 91, 5, 5, 5, 1, 1], [1, 30.0, 15.5, 70, 2, 4, 3, 1, 3], [2, 61.0, 18.5, 114, 5, 9, 5, 2, 3], [3, 66.0, 24.3, 72, 3, 5, 5, 1, 3], [3, 59.0, 24.0, 61, 4, 4, 5, 1, 3], [3, 59.0, 24.3, 67, 3, 4, 5, 1, 3], [3, 59.0, 23.0, 77, 4, 5, 2, 1, 3], [3, 130.0, 87.0, 119, 2, 3, 6, 1, 2], [3, 66.0, 25.5, 86, 4, 5, 5, 1, 3], [3, 64.0, 35.5, 80, 4, 5, 6, 1, 1], [1, 48.0, 17.0, 117, 7, 9, 3, 1, 2], [3, 60.0, 28.5, 87, 3, 5, 6, 1, 3], [1, 33.5, 12.0, 85, 4, 5, 5, 1, 1], [2, 55.0, 25.7, 77, 4, 4, 5, 4, 1], [1, 33.0, 16.6, 71, 4, 5, 5, 1, 3], [5, 140.2, 119.9, 92, 2, 14, 6, 4, 2], [1, 48.0, 16.9, 105, 7, 9, 5, 1, 2], [2, 45.0, 17.9, 85, 3, 5, 5, 1, 3], [1, 35.0, 12.5, 85, 5, 5, 5, 1, 3], [2, 36.0, 10.5, 63, 2, 2, 5, 1, 1], [2, 70.0, 22.3, 115, 3, 5, 5, 1, 2], [3, 84.0, 31.0, 88, 2, 9, 5, 2, 3], [2, 45.4, 25.0, 76, 4, 5, 6, 1, 3], [3, 112.0, 55.0, 78, 9, 9, 6, 3, 3], [2, 49.6, 19.5, 87, 5, 5, 5, 4, 3], [2, 45.0, 19.9, 58, 2, 2, 3, 1, 1], [2, 68.0, 42.0, 112, 2, 18, 6, 1, 2], [2, 65.0, 30.0, 112, 8, 18, 5, 1, 2], [3, 70.0, 58.0, 80, 4, 5, 6, 1, 2], [2, 68.0, 29.9, 111, 5, 18, 5, 1, 2], [2, 54.0, 27.0, 112, 8, 18, 6, 1, 2], [2, 70.0, 33.0, 111, 14, 18, 6, 1, 2], [2, 68.0, 35.5, 114, 2, 18, 6, 1, 2], [2, 76.0, 32.0, 112, 18, 18, 6, 1, 2], [6, 234.0, 195.0, 119, 2, 3, 4, 1, 1], [2, 43.0, 21.777777, 65, 4, 4, 6, 1, 3], [3, 74.0, 36.0, 105, 2, 9, 6, 1, 2], [2, 60.0, 31.0, 85, 2, 9, 5, 3, 3], [2, 57.0, 28.0, 87, 7, 9, 5, 4, 3], [2, 43.0, 23.5, 78, 4, 5, 5, 1, 3], [3, 78.0, 33.45, 101, 9, 9, 3, 2, 1], [2, 58.0, 27.0, 89, 9, 9, 5, 2, 3], [2, 52.0, 30.5, 79, 2, 5, 6, 4, 1], [3, 61.0, 27.0, 74, 4, 5, 6, 1, 3], [2, 62.6, 17.528, 122, 1, 4, 1, 1, 2], [1, 35.0, 14.0, 121, 9, 9, 6, 2, 2], [2, 53.0, 34.0, 81, 5, 5, 6, 1, 1], [1, 31.7, 18.2, 80, 2, 4, 5, 2, 3], [2, 61.0, 29.0, 111, 10, 18, 6, 2, 2], [1, 31.0, 20.4, 60, 3, 4, 6, 1, 1], [1, 36.0, 17.5, 81, 4, 5, 3, 1, 1], [2, 42.0, 21.0, 83, 2, 5, 5, 1, 3], [2, 53.2, 28.8, 91, 5, 11, 3, 3, 1], [3, 81.0, 34.95, 92, 3, 9, 5, 2, 3], [3, 60.0, 22.4, 63, 3, 4, 5, 1, 3], [3, 83.0, 29.5, 110, 5, 9, 5, 3, 2], [1, 39.0, 19.9, 85, 5, 5, 5, 1, 3], [2, 51.0, 28.0, 82, 5, 5, 6, 3, 1], [2, 63.0, 28.0, 115, 3, 10, 6, 1, 2], [1, 31.0, 16.5, 71, 3, 4, 6, 1, 3], [2, 45.0, 25.5, 79, 5, 5, 5, 1, 3], [2, 52.4, 30.0, 86, 2, 9, 5, 2, 3], [3, 52.4, 22.0, 52, 2, 2, 5, 1, 1], [3, 62.0, 33.0, 74, 5, 5, 5, 1, 3], [2, 50.4, 22.5, 91, 5, 5, 6, 4, 1], [1, 36.4, 22.0, 76, 5, 5, 5, 2, 1], [4, 74.0, 50.0, 78, 12, 12, 6, 2, 2], [2, 50.0, 29.0, 80, 4, 5, 5, 3, 1], [2, 45.0, 20.0, 69, 4, 4, 5, 1, 1], [2, 42.0, 21.0, 60, 3, 4, 5, 1, 1], [2, 43.0, 18.0, 66, 2, 4, 5, 1, 3], [3, 91.3, 55.0, 117, 3, 3, 6, 3, 1], [3, 66.0, 24.3, 72, 3, 5, 5, 1, 3], [3, 58.0, 24.3, 67, 3, 4, 5, 1, 3], [1, 40.0, 25.8, 70, 3, 5, 5, 1, 1], [4, 98.0, 53.0, 77, 2, 7, 6, 4, 3], [2, 53.0, 30.0, 91, 5, 11, 5, 2, 1], [3, 58.0, 22.0, 70, 4, 4, 5, 1, 3], [1, 33.0, 18.8, 78, 3, 5, 6, 1, 3], [3, 68.7, 33.5, 84, 4, 9, 5, 1, 2], [2, 39.4, 20.0, 63, 3, 4, 6, 2, 1], [1, 31.0, 16.5, 67, 4, 4, 5, 1, 1], [3, 73.0, 31.9, 89, 7, 9, 5, 2, 3], [1, 44.6, 14.9, 115, 6, 9, 5, 1, 2], [2, 49.0, 26.0, 122, 7, 17, 4, 2, 2], [3, 71.1, 28.0, 115, 9, 9, 5, 1, 2], [1, 20.0, 9.5, 69, 2, 4, 5, 1, 1], [2, 46.0, 21.5, 86, 4, 5, 5, 1, 3], [3, 64.0, 20.0, 76, 2, 2, 5, 1, 1], [2, 42.0, 23.0, 62, 3, 4, 5, 1, 1], [2, 56.0, 25.5, 105, 3, 9, 6, 1, 3], [4, 137.0, 83.0, 111, 12, 16, 6, 1, 2], [3, 60.0, 25.0, 66, 3, 4, 5, 1, 3], [2, 41.0, 17.5, 62, 3, 4, 5, 1, 1], [1, 40.0, 22.0, 87, 1, 5, 2, 1, 3], [3, 65.0, 25.0, 82, 3, 5, 5, 4, 3], [1, 32.3, 16.2, 66, 3, 4, 5, 1, 3], [3, 83.9, 30.0, 118, 8, 9, 5, 1, 2], [2, 60.0, 26.0, 76, 7, 8, 5, 3, 3], [2, 44.0, 20.0, 76, 4, 5, 3, 1, 3]]\n",
    "ys = [[0, 0, 0, 1, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1], [0, 0, 1, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 1], [1, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 1], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 1], [1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 1], [0, 0, 1, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 0, 0, 1, 0], [1, 0, 0, 0, 0], [0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 1], [0, 0, 1, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 1, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 0, 0, 1, 0], [1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1], [0, 1, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 1, 0, 0, 0], [0, 1, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 1], [1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1], [0, 1, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1, 0, 0], [1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 1, 0, 0], [0, 0, 0, 0, 1], [0, 1, 0, 0, 0], [0, 0, 0, 0, 1], [0, 1, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 1, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1], [1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [1, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1], [0, 1, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 1, 0], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 1, 0, 0, 0], [1, 0, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 1], [0, 0, 1, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1], [0, 0, 1, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 0, 0, 1], [0, 0, 1, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 1, 0], [0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 0, 1], [0, 1, 0, 0, 0], [0, 1, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 1, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 1, 0], [1, 0, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 1, 0, 0], [0, 1, 0, 0, 0], [0, 1, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 1, 0], [0, 1, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 0, 1, 0], [0, 0, 1, 0, 0], [0, 0, 0, 1, 0], [0, 0, 0, 0, 1], [1, 0, 0, 0, 0], [0, 0, 1, 0, 0], [0, 1, 0, 0, 0], [0, 1, 0, 0, 0], [0, 0, 1, 0, 0], [0, 0, 0, 0, 1], [0, 0, 1, 0, 0], [0, 1, 0, 0, 0], [1, 0, 0, 0, 0], [0, 0, 0, 1, 0], [1, 0, 0, 0, 0], [0, 0, 0, 0, 1], [0, 0, 0, 1, 0], [0, 1, 0, 0, 0]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 3.  58.5 29.9 ...  6.   1.   1. ]\n",
      " [ 1.  33.  16.2 ...  5.   1.   3. ]\n",
      " [ 2.  50.  21.5 ...  3.   1.   3. ]\n",
      " ...\n",
      " [ 3.  83.9 30.  ...  5.   1.   2. ]\n",
      " [ 2.  60.  26.  ...  5.   3.   3. ]\n",
      " [ 2.  44.  20.  ...  3.   1.   3. ]]\n",
      "[[0 0 0 1 0]\n",
      " [1 0 0 0 0]\n",
      " [1 0 0 0 0]\n",
      " ...\n",
      " [0 0 0 0 1]\n",
      " [0 0 0 1 0]\n",
      " [0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "xs = np.array(xs)\n",
    "ys = np.array(ys)\n",
    "print(xs)\n",
    "print(ys)\n",
    "train_x = xs\n",
    "train_y = ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, lr, batch_size, hidden_layers = 5, 1e-2, 300, 100\n",
    "in_size = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gen_rand(X, y, batch_size=batch_size):\n",
    "    idx = np.random.randint(X.shape[0], size=batch_size)\n",
    "    X_batch = X[idx]\n",
    "    y_batch = y[idx]\n",
    "    return Variable(torch.FloatTensor(X_batch)), Variable(torch.LongTensor(y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_batch, y_batch = batch_gen_rand(xs, ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNN(nn.Module):\n",
    "    def __init__(self, in_size, h_layers, num_classes):\n",
    "        super(MyNN, self).__init__()\n",
    "        self.input = nn.Linear(in_features = in_size, out_features = h_layers)\n",
    "        self.relu_1 = nn.Sigmoid()\n",
    "        self.hidden_1 = nn.Linear(in_features = h_layers, out_features = h_layers)\n",
    "        self.relu_2 = nn.Sigmoid()\n",
    "        self.output = nn.Linear(in_features = h_layers, out_features = num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x1 = self.input(x)\n",
    "        x2 = self.relu_1(x1)\n",
    "        x3 = self.hidden_1(x2)\n",
    "        x4 = self.relu_2(x3)\n",
    "        return self.output(x4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNN(in_size, hidden_layers, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user1\\anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "1D target tensor expected, multi-target not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-5c9c9dea0e5a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0my_predicted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_predicted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[0;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1045\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1047\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m   1048\u001b[0m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0;32m   1049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2691\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2692\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2693\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2694\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2386\u001b[0m         )\n\u001b[0;32m   2387\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2388\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2389\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2390\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: 1D target tensor expected, multi-target not supported"
     ]
    }
   ],
   "source": [
    "# create dummy data with 3 samples and 784 features\n",
    "x_batch = torch.tensor(xs[:3], dtype=torch.float32)\n",
    "y_batch = torch.tensor(ys[:3], dtype=torch.long)\n",
    "\n",
    "# compute outputs given inputs, both are variables\n",
    "y_predicted = model(x_batch)\n",
    "\n",
    "loss = criterion(y_predicted, y_batch)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING\n",
    "x = torch.FloatTensor(xs)\n",
    "y = torch.FloatTensor(ys)\n",
    "print(x)\n",
    "print(y)\n",
    "for e in range(10):\n",
    "    for i in range(0, len(xs)):\n",
    "        x_batch, y_batch = batch_gen_rand(xs, ys)\n",
    "        output = model(x_batch)\n",
    "        loss = criterion(output, y_batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print('epoch: {} step: {} loss: {}'.format(e, i, loss.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "\n",
    "def max_index(l):\n",
    "    maxi = l[0]\n",
    "    maxi_index = 0\n",
    "    for i in range(len(l)):\n",
    "        if l[i] > maxi:\n",
    "            maxi = l[i]\n",
    "            maxi_index = i\n",
    "    return maxi_index\n",
    "\n",
    "for t in range(int(10000/batch_size)):\n",
    "    x_batch, y_batch = batch_gen_rand(train_x, train_y)\n",
    "    y_pred = model(x_batch)\n",
    "    for k in range(batch_size):\n",
    "        print(y_pred[k].tolist())\n",
    "        print('{} {}'.format(max_index(y_pred[k].tolist()), y_batch[k]))\n",
    "        if train_y[k] == max_index(y_pred[k].tolist()):\n",
    "            cnt += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
